# XML to SQL Converter - Consolidated Research Brief

**Version**: 3.0.0
**Date**: 2025-11-21
**Purpose**: Comprehensive pre-research context for external Claude agent
**Est. Reading Time**: 45-60 minutes

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Project Overview](#project-overview)
3. [Current Implementation Status](#current-implementation-status)
4. [Architecture Deep Dive](#architecture-deep-dive)
5. [Intermediate Representation Design](#intermediate-representation-design)
6. [Conversion Logic & Rules](#conversion-logic--rules)
7. [Multi-Database Support Strategy](#multi-database-support-strategy)
8. [Package Mapping System](#package-mapping-system)
9. [Validation & Quality Assurance](#validation--quality-assurance)
10. [Technical Challenges Solved](#technical-challenges-solved)
11. [Extension Opportunities](#extension-opportunities)
12. [Reference Documentation](#reference-documentation)

---

## Executive Summary

**What is This Project?**

A production-ready Python tool that converts SAP HANA Calculation View XML definitions into executable SQL for multiple database platforms (HANA native SQL and Snowflake), enabling data migration, system consolidation, and platform independence.

**Key Statistics** (as of 2025-11-20):
- **100% Success Rate**: 8/8 XMLs validated in HANA execution
- **Production XMLs Tested**: 9 real-world calculation views from ECC and BW systems
- **Bugs Resolved**: 23 bugs documented and solved with full solutions
- **Execution Performance**: 29ms to 243ms per view (HANA Studio validation)
- **Database Targets**: SAP HANA (native), Snowflake (cloud warehouse), future: Databricks, PostgreSQL

**Unique Value Proposition**:
1. **Dual-Mode Architecture**: Single codebase generates both HANA and Snowflake SQL
2. **Package Mapping System**: Automatic resolution of HANA CV paths from metadata (Excel/DB)
3. **100% Validation**: All generated SQL verified by actual HANA execution
4. **IR-Based Independence**: Intermediate representation enables extensibility to any SQL database

**What Makes It Different**:
- Not a code generator - it's a semantic translator with deep understanding of HANA CV logic
- Maintains calculation view semantics while adapting to target database idioms
- Real-world validated on production

 data (not theoretical/academic)
- Complete traceability: XML ‚Üí IR ‚Üí SQL with debug capabilities

---

## Project Overview

### Problem Statement

**Business Context**:
Organizations running SAP ERP systems store critical analytics in **SAP HANA Calculation Views** - complex XML-defined virtual models combining multiple data sources with business logic. When migrating to cloud warehouses (Snowflake, Databricks) or consolidating HANA systems, these views must be recreated as native SQL.

**Technical Challenge**:
- **Manual Translation**: Hand-coding SQL from XML is error-prone and time-consuming
- **Semantic Complexity**: Calculation views encode business rules, hierarchies, currency conversions, and analytic privileges
- **Platform Differences**: HANA SQL ‚â† Snowflake SQL (function names, syntax, data types)
- **Scale Problem**: Enterprises have 100s or 1000s of calculation views

### Solution Architecture

```
SAP HANA Calculation View XML
           ‚Üì
    [XML Parser]
           ‚Üì
Intermediate Representation (IR)
           ‚Üì
    [SQL Renderer]
           ‚Üì
  Database-Specific SQL
  (HANA or Snowflake)
```

**Key Innovation**: The **Intermediate Representation** decouples XML parsing from SQL generation, enabling:
- Single parser handles all XML variants (Calculation:scenario, ColumnView)
- Multiple SQL renderers target different databases
- Extension to new databases without touching parser
- Test/debug at IR level independently

### Supported Features

**‚úÖ Fully Implemented**:
- **Node Types**: Projection, Join (Inner/Left/Right/Full Outer), Aggregation, Union, Rank
- **Expressions**: Calculated columns with formula translation
- **Filters**: WHERE clause generation with predicate translation
- **Functions**: 30+ HANA functions mapped to target databases
- **Data Sources**: Tables, views, other calculation views
- **Logical Model**: Measures and attributes with aggregation types
- **Parameter Handling**: `$$client$$`, `$$language$$` placeholder substitution
- **Configuration**: YAML-based with runtime overrides

**‚è≥ Partially Implemented**:
- **Currency Conversion**: Structure parsed, UDF call generation (full conversion logic TBD)
- **Hierarchies**: XML parsing ready, SQL generation pending
- **Analytic Privileges**: Filter extraction working, SQL propagation pending

**üîú Future Scope**:
- **Variables**: Input parameter support
- **Time-based calculations**: Fiscal period functions
- **Restricted columns**: Security propagation

### Use Cases

1. **Cloud Migration**: Lift-and-shift HANA analytics to Snowflake
2. **System Consolidation**: Merge multiple SAP instances
3. **Platform Independence**: Generate SQL for any database
4. **Disaster Recovery**: Export calculation views as portable SQL
5. **Dev/Test Environments**: Run analytics without full HANA license
6. **Documentation**: Auto-generate SQL documentation from XMLs

---

## Current Implementation Status

### Latest Validation Results (Session 8, 2025-11-20)

**Validated XMLs** (All executed successfully in HANA Studio):

| XML File | Instance | Size | Exec Time | Status | Session |
|----------|----------|------|-----------|--------|---------|
| CV_CNCLD_EVNTS.xml | ECC | 243 lines | 84ms | ‚úÖ | 7 |
| CV_INVENTORY_ORDERS.xml | BW | 220 lines | 34ms | ‚úÖ | 7 |
| CV_PURCHASE_ORDERS.xml | BW | ~220 lines | 29ms | ‚úÖ | 7 |
| CV_EQUIPMENT_STATUSES.xml | BW | 170 lines | 32ms | ‚úÖ | 7 |
| CV_MCM_CNTRL_Q51.xml | ECC | - | 82ms | ‚úÖ | 7 |
| CV_MCM_CNTRL_REJECTED.xml | ECC | - | 53ms | ‚úÖ | 7 |
| CV_CT02_CT03.xml | ECC | - | 39ms | ‚úÖ | 7 |
| CV_COMMACT_UNION.xml | BW/MBD | - | 36ms | ‚úÖ | 8 |

**Success Rate**: 8/8 = **100%**

**Complexity Profile**:
- **Simple**: 3 views (< 200 lines, 1-2 joins)
- **Medium**: 4 views (200-500 lines, 3-5 joins)
- **Complex**: 1 view (2000+ lines, 10+ joins, unions, ranks)

### Development Milestones

**Phase 1: Foundation** (Completed 2025-11-13)
- Core XML parser with IR models
- Snowflake SQL renderer
- CLI interface
- Basic function translation catalog

**Phase 2: HANA Mode** (Completed 2025-11-16)
- HANA SQL renderer (native views)
- Version-aware generation (1.0, 2.0, SPS01, SPS03)
- Legacy function rewrites (IF‚ÜíCASE, IN‚ÜíOR)
- Schema qualification
- ColumnView XML support

**Phase 3: Package Mapping** (Completed 2025-11-19)
- Excel/DB metadata import
- Automatic CV path resolution
- Web API integration
- 100% auto-detection accuracy

**Phase 4: Web Interface** (Completed 2025-11-13)
- React frontend with FastAPI backend
- Single/batch conversion
- History with SQLite database
- Validation results display
- Auto-correction engine

**Phase 5: Production Validation** (Ongoing 2025-11-16 to 2025-11-20)
- Multi-instance testing (ECC, BW, MBD, BID)
- 8 XMLs validated at 100% success rate
- 23 bugs discovered and solved
- Performance: 29ms-243ms per view

### Features Under Active Development

**Current Focus**:
- Testing more XMLs from different SAP instances
- Expanding function translation catalog
- Improving error messages and diagnostics
- Performance optimization for large XMLs (2000+ lines)

**Pending Bugs** (6 active):
- BUG-019: Complex calculated column rendering
- BUG-023: Package path handling edge cases
- BUG-024: Multi-level aggregation
- BUG-025: Calculation view reference validation
- BUG-026: String column type inference
- BUG-002, BUG-003: Parameter pattern cleanup (deferred)

---

## Architecture Deep Dive

### High-Level Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     XML PARSING PHASE                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
Input: SAP HANA Calculation View XML (Calculation:scenario or ColumnView)
                              ‚Üì
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ         XML Parser (lxml-based)         ‚îÇ
         ‚îÇ  - Namespace normalization              ‚îÇ
         ‚îÇ  - XPath queries for nodes/attributes   ‚îÇ
         ‚îÇ  - Dependency graph construction        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              INTERMEDIATE REPRESENTATION (IR)                ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Scenario (root)                                             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ DataSources (tables, views, CVs)                       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Nodes (topologically ordered)                          ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ ProjectionNode                                     ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ JoinNode                                          ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ AggregationNode                                   ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ UnionNode                                         ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ RankNode                                          ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ LogicalModel (measures, attributes)                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Parameters (client, language, etc.)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   SQL GENERATION PHASE                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ        SQL Renderer (mode-aware)        ‚îÇ
         ‚îÇ  - Database mode selection              ‚îÇ
         ‚îÇ  - CTE generation (topological order)   ‚îÇ
         ‚îÇ  - Expression translation               ‚îÇ
         ‚îÇ  - Function catalog application         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
Output: Database-Specific SQL (HANA or Snowflake)
```

### Component Breakdown

#### 1. XML Parser (`src/xml_to_sql/parser/`)

**Responsibility**: Convert XML to IR models

**Two Parser Strategies**:

**A. Scenario Parser** (`scenario_parser.py`):
- Handles `Calculation:scenario` namespace XMLs
- Most common in SAP BI projects
- XPath: `//calculationViews/calculationView[@xsi:type]`

**B. ColumnView Parser** (`column_view_parser.py`):
- Handles legacy `ColumnView` namespace XMLs
- Common in older HANA installations
- XPath: `//columnView/viewNode[@xsi:type]`

**Parsing Steps**:
1. **Load XML**: `lxml.etree.parse()` with namespace awareness
2. **Extract Metadata**: Scenario ID, default client/language
3. **Register Data Sources**: Parse `<dataSources>` ‚Üí `DataSource` objects
4. **Parse Nodes**: Iterate `<calculationView>` elements:
   - Dispatch by `xsi:type` (Projection, Join, Aggregation, Union, Rank)
   - Extract inputs, mappings, calculated columns, filters
   - Build dependency graph
5. **Topological Sort**: Kahn's algorithm ensures CTE generation order
6. **Logical Model**: Parse `<logicalModel>` ‚Üí measures/attributes
7. **Validation**: Check references, detect cycles

**Key Classes**:
- `Scenario`: Root IR object
- `DataSource`: Table/view/CV reference
- `Node` (abstract): Base for all node types
- `Expression`: Column refs, literals, function calls
- `Predicate`: Filter conditions

#### 2. Intermediate Representation (`src/xml_to_sql/domain/`)

**Philosophy**: Database-agnostic semantic model

**Core Design Principles**:
1. **Decoupling**: XML parsing ‚â† SQL generation
2. **Serializable**: JSON/YAML export for debugging
3. **Extensible**: Add new node types without breaking existing code
4. **Testable**: Unit test IR ‚Üí SQL independently

**IR Hierarchy**:

```python
@dataclass
class Scenario:
    id: str
    metadata: dict
    data_sources: dict[str, DataSource]
    nodes: OrderedDict[str, Node]
    logical_model: LogicalModel

@dataclass
class DataSource:
    id: str
    schema: str
    object_name: str
    type: DataSourceType  # TABLE, VIEW, CALCULATION_VIEW
    columns: list[str]

@dataclass
class Node:
    id: str
    type: NodeType  # PROJECTION, JOIN, AGGREGATION, UNION, RANK
    inputs: list[str]  # References to data sources or other nodes
    mappings: dict[str, Expression]
    filters: list[Predicate]

@dataclass
class Expression:
    type: ExpressionType  # COLUMN_REF, LITERAL, FUNCTION_CALL, CASE_WHEN
    value: Any
    metadata: dict

@dataclass
class Predicate:
    kind: PredicateKind  # COMPARISON, BETWEEN, IN_LIST, IS_NULL
    left: Expression
    operator: str
    right: Expression
```

**Relationships**:
- Nodes reference other nodes by ID
- Logical model attributes reference node outputs
- Currency conversions reference measures

**Benefits**:
- Single IR, multiple SQL renderers
- Test conversions at semantic level
- Debug by inspecting IR JSON
- Cache/serialize IR for performance

#### 3. SQL Renderer (`src/xml_to_sql/sql/renderer.py`)

**Responsibility**: Transform IR to SQL

**Mode-Aware Rendering**:

```python
def render_sql(scenario: Scenario, config: Config) -> str:
    if config.database_mode == "hana":
        return _render_hana_sql(scenario, config)
    elif config.database_mode == "snowflake":
        return _render_snowflake_sql(scenario, config)
```

**Rendering Steps**:

1. **Context Setup**:
   - Load configuration (schema overrides, parameters)
   - Initialize symbol table (node ID ‚Üí CTE alias)
   - Resolve parameter values (`$$client$$` ‚Üí actual value)

2. **CTE Generation** (for each node in topological order):
   ```sql
   WITH
     projection_1 AS (
       SELECT ... FROM data_source WHERE ...
     ),
     join_1 AS (
       SELECT ... FROM projection_1 JOIN another_source ON ...
     ),
     aggregation_1 AS (
       SELECT ... FROM join_1 GROUP BY ...
     )
   ```

3. **Expression Translation**:
   - `ExpressionRenderer` converts IR expressions to SQL
   - Function catalog applies rewrites (catalog-driven)
   - Mode-specific transformations (IF‚ÜíIFF for Snowflake, IF‚ÜíCASE for HANA)

4. **Final SELECT**:
   ```sql
   SELECT * FROM aggregation_1
   ```

5. **View Wrapping** (HANA mode):
   ```sql
   DROP VIEW "_SYS_BIC"."Package.Path/ViewName" CASCADE;
   CREATE VIEW "_SYS_BIC"."Package.Path/ViewName" AS
   WITH ... SELECT * FROM ...
   ```

**Critical Functions**:
- `_render_projection()`: Simple SELECT with column mappings
- `_render_join()`: JOIN clauses with condition translation
- `_render_aggregation()`: GROUP BY with aggregate functions
- `_render_union()`: UNION/UNION ALL with column alignment
- `_render_rank()`: Window functions (ROW_NUMBER, RANK)

#### 4. Function Translator (`src/xml_to_sql/sql/function_translator.py`)

**Responsibility**: HANA ‚Üí Target DB function mapping

**Two-Phase Translation**:

**Phase 1: Pattern Matching** (`patterns.yaml`):
- Regex-based expression rewrites
- Applied FIRST in pipeline
- Example: `NOW() - 365` ‚Üí `ADD_DAYS(CURRENT_DATE, -365)`

**Phase 2: Function Catalog** (`functions.yaml`):
- Simple name mapping
- Applied SECOND in pipeline
- Example: `string(x)` ‚Üí `TO_VARCHAR(x)`

**Translation Pipeline**:
```
Raw Formula
    ‚Üì
[Pattern Rewrites]
    ‚Üì
[Function Catalog]
    ‚Üì
[Mode-Specific Transforms]
    ‚Üì
Output SQL
```

**Catalog Entry Example**:
```yaml
- name: STRING
  handler: rename
  target: "TO_VARCHAR"
  description: "Legacy string() cast to VARCHAR"

- name: IF
  handler: mode_specific
  hana: template: "CASE WHEN {0} THEN {1} ELSE {2} END"
  snowflake: template: "IFF({0}, {1}, {2})"
```

**Handler Types**:
- `rename`: Simple name change
- `template`: Format string with arg substitution
- `mode_specific`: Different templates per database

#### 5. Package Mapper (`src/xml_to_sql/package_mapper.py`)

**Responsibility**: Resolve HANA CV package paths automatically

**Problem Solved**:
- HANA Calculation Views stored in package hierarchy: `Macabi_BI.Eligibility`
- Generated SQL must reference: `"_SYS_BIC"."Macabi_BI.Eligibility/CV_NAME"`
- Manual specification error-prone

**Solution**:
- Import metadata from Excel/DB: `CV_NAME ‚Üí Package.Path`
- Singleton mapper caches mappings
- Auto-detect package from filename

**Data Flow**:
```
metadata/excel_export.xlsx
         ‚Üì
   [Excel Parser]
         ‚Üì
package_mappings.json
         ‚Üì
 [PackageMapper]
         ‚Üì
CV_NAME ‚Üí "Macabi_BI.Eligibility"
```

**Usage**:
```python
from package_mapper import get_package

package = get_package("CV_CNCLD_EVNTS")
# Returns: "EYAL.EYAL_CTL"

# Generate SQL:
# "_SYS_BIC"."EYAL.EYAL_CTL/CV_CNCLD_EVNTS"
```

**Benefits**:
- 100% accuracy (validated against 8 XMLs)
- Zero manual configuration
- Supports multiple SAP instances

#### 6. Web Interface

**Backend** (`src/xml_to_sql/web/`):
- FastAPI application
- SQLite database for history
- REST API endpoints
- Validation & auto-correction

**Frontend** (`web_frontend/`):
- React + Vite
- Split/tab view for XML/SQL
- Configuration UI
- History management
- Validation results display

---

## Intermediate Representation Design

### Goals

1. **Decoupling**: Separate XML parsing from SQL generation
2. **Database Independence**: IR represents semantics, not syntax
3. **Extensibility**: Add new node types without breaking existing code
4. **Testability**: Validate conversions at semantic level
5. **Serializability**: Export IR as JSON/YAML for debugging

### Core Entities

#### Scenario

**Purpose**: Root object representing entire calculation view

**Attributes**:
- `id`: Calculation view identifier (e.g., "CV_CNCLD_EVNTS")
- `metadata`: Default client, language, privilege flags
- `data_sources`: Map of source ID ‚Üí `DataSource`
- `nodes`: Ordered dict of node ID ‚Üí `Node` subclass
- `logical_model`: Exposed measures/attributes

**Example**:
```json
{
  "id": "CV_CNCLD_EVNTS",
  "metadata": {
    "default_client": "800",
    "default_language": "E"
  },
  "data_sources": {
    "VBAK": {
      "schema": "SAPABAP1",
      "object_name": "VBAK",
      "type": "TABLE"
    }
  },
  "nodes": {
    "Projection_1": { ... },
    "Join_1": { ... }
  }
}
```

#### DataSource

**Purpose**: Reference to table, view, or calculation view

**Attributes**:
- `id`: Source identifier (e.g., "VBAK", "CV_ELIG_TRANS_01")
- `schema`: Database schema (e.g., "SAPABAP1", "_SYS_BIC")
- `object_name`: Table/view name
- `type`: `TABLE`, `VIEW`, `CALCULATION_VIEW`
- `columns`: Optional explicit column list

**Example**:
```json
{
  "id": "VBAK",
  "schema": "SAPABAP1",
  "object_name": "VBAK",
  "type": "TABLE",
  "columns": ["VBELN", "ERDAT", "KUNNR"]
}
```

#### Node (Abstract Base Class)

**Purpose**: Represents a calculation step (projection, join, etc.)

**Common Attributes** (all nodes):
- `id`: Node identifier (e.g., "Projection_1")
- `type`: `PROJECTION`, `JOIN`, `AGGREGATION`, `UNION`, `RANK`
- `inputs`: List of input references (data source IDs or node IDs)
- `mappings`: Output column name ‚Üí `Expression`
- `filters`: List of `Predicate` objects (WHERE clauses)
- `properties`: Node-specific metadata

#### Node Types

**1. ProjectionNode**

**Purpose**: Column selection and calculated columns

**Attributes**:
- `inputs`: Single data source or node
- `mappings`: Column renames and formulas

**SQL Output**:
```sql
projection_1 AS (
  SELECT
    VBAK.VBELN AS ORDER_ID,
    VBAK.ERDAT AS CREATED_DATE,
    VBAK.KUNNR AS CUSTOMER_ID
  FROM SAPABAP1.VBAK
  WHERE VBAK.BUKRS = '1000'
)
```

**2. JoinNode**

**Purpose**: Combine two data sources

**Attributes**:
- `inputs`: Two data sources/nodes (left, right)
- `join_type`: `INNER`, `LEFT_OUTER`, `RIGHT_OUTER`, `FULL_OUTER`
- `conditions`: List of `JoinCondition` (left attr, op, right attr)
- `calculated_attributes`: Optional derived columns in join

**SQL Output**:
```sql
join_1 AS (
  SELECT
    projection_1.*,
    projection_2.CUSTOMER_NAME
  FROM projection_1
  INNER JOIN projection_2 ON projection_1.CUSTOMER_ID = projection_2.KUNNR
)
```

**3. AggregationNode**

**Purpose**: GROUP BY with aggregate functions

**Attributes**:
- `group_by`: List of grouping columns
- `aggregations`: Output name ‚Üí aggregation expression (SUM, MAX, etc.)

**SQL Output**:
```sql
aggregation_1 AS (
  SELECT
    join_1.CUSTOMER_ID,
    SUM(join_1.AMOUNT) AS TOTAL_AMOUNT,
    MAX(join_1.CREATED_DATE) AS LATEST_DATE
  FROM join_1
  GROUP BY join_1.CUSTOMER_ID
)
```

**4. UnionNode**

**Purpose**: Combine multiple node outputs vertically

**Attributes**:
- `inputs`: List of 2+ nodes
- `union_type`: `UNION` or `UNION_ALL`
- `column_mapping`: Align columns across inputs

**SQL Output**:
```sql
union_1 AS (
  SELECT ORDER_ID, AMOUNT FROM projection_1
  UNION ALL
  SELECT ORDER_ID, AMOUNT FROM projection_2
)
```

**5. RankNode**

**Purpose**: Window functions (ROW_NUMBER, RANK, DENSE_RANK)

**Attributes**:
- `partition_by`: List of partition columns
- `order_by`: List of sort columns
- `rank_function`: `ROW_NUMBER`, `RANK`, `DENSE_RANK`
- `threshold`: Optional row limit (TOP N)

**SQL Output**:
```sql
rank_1 AS (
  SELECT
    *,
    ROW_NUMBER() OVER (PARTITION BY CUSTOMER_ID ORDER BY AMOUNT DESC) AS RN
  FROM join_1
)
-- Apply threshold filter
SELECT * FROM rank_1 WHERE RN <= 10
```

#### Expression

**Purpose**: Represent column references, literals, function calls

**Attributes**:
- `type`: `COLUMN_REF`, `LITERAL`, `FUNCTION_CALL`, `CASE_WHEN`, `ARITHMETIC`
- `value`: Payload depends on type
- `metadata`: Hints (original function, required casts)

**Examples**:
```json
// Column reference
{
  "type": "COLUMN_REF",
  "value": "VBAK.VBELN"
}

// Literal
{
  "type": "LITERAL",
  "value": "'1000'"
}

// Function call
{
  "type": "FUNCTION_CALL",
  "value": {
    "name": "RIGHT",
    "args": [
      {"type": "COLUMN_REF", "value": "CALMONTH"},
      {"type": "LITERAL", "value": "2"}
    ]
  }
}

// Arithmetic
{
  "type": "ARITHMETIC",
  "value": {
    "operator": "+",
    "left": {"type": "COLUMN_REF", "value": "AMOUNT"},
    "right": {"type": "LITERAL", "value": "100"}
  }
}
```

#### Predicate

**Purpose**: Represent filter conditions

**Attributes**:
- `kind`: `COMPARISON`, `BETWEEN`, `IN_LIST`, `IS_NULL`, `LIKE`
- `left`: `Expression`
- `operator`: `=`, `<>`, `<`, `>`, `<=`, `>=`, `BETWEEN`, `IN`, `IS NULL`, `LIKE`
- `right`: `Expression` (or list for `IN`)

**Examples**:
```json
// Comparison
{
  "kind": "COMPARISON",
  "left": {"type": "COLUMN_REF", "value": "BUKRS"},
  "operator": "=",
  "right": {"type": "LITERAL", "value": "'1000'"}
}

// IN list
{
  "kind": "IN_LIST",
  "left": {"type": "COLUMN_REF", "value": "STATUS"},
  "operator": "IN",
  "right": [
    {"type": "LITERAL", "value": "'OPEN'"},
    {"type": "LITERAL", "value": "'PENDING'"}
  ]
}
```

### Extensibility Hooks

**1. NodeFactory Pattern**:
```python
class NodeFactory:
    @staticmethod
    def create_node(xml_element) -> Node:
        node_type = xml_element.get("{http://www.w3.org/2001/XMLSchema-instance}type")

        if node_type == "Calculation:ProjectionView":
            return ProjectionNode.from_xml(xml_element)
        elif node_type == "Calculation:JoinView":
            return JoinNode.from_xml(xml_element)
        # ... etc
```

**2. ExpressionFactory Pattern**:
```python
class ExpressionFactory:
    @staticmethod
    def parse_formula(formula_str: str) -> Expression:
        # Pattern matching, catalog lookup, etc.
        pass
```

**3. Reserved Fields for Future**:
- Analytic privilege propagation
- Hierarchies (parent-child)
- Variables (input parameters)
- Currency conversion metadata

### Serialization

**JSON Export**:
```python
import json

scenario = parse_xml("CV_CNCLD_EVNTS.xml")
ir_json = json.dumps(scenario.to_dict(), indent=2)

# Save for debugging
with open("debug_ir.json", "w") as f:
    f.write(ir_json)
```

**Benefits**:
- Debug conversion issues by inspecting IR
- Unit test: XML ‚Üí IR ‚Üí assert IR structure
- Cache IR to avoid re-parsing
- Version control IR for regression testing

---

## Conversion Logic & Rules

### HANA Conversion Rules (Top 10 Critical Rules)

#### Rule 1: IF() to CASE WHEN (Priority 40)

**Context**: HANA SQL views don't support `IF()` function in SELECT clauses

**Transformation**:
```sql
-- Before (XML formula)
IF(RIGHT("CALMONTH", 2) = '01', '2015' + '1', NULL)

-- After (HANA SQL)
CASE WHEN RIGHT("CALMONTH", 2) = '01' THEN '2015' + '1' ELSE NULL END
```

**Implementation**: `function_translator.py::_convert_if_to_case_for_hana()`

**Validated**: 12 IF statements in CV_CNCLD_EVNTS.xml

#### Rule 2: IN Operator to OR Conditions (Priority 30)

**Context**: Legacy ColumnView XMLs use `in(value1, value2, ...)` helper

**Transformation**:
```sql
-- Before
in("STATUS", '01', '02', '03')

-- After
("STATUS" = '01' OR "STATUS" = '02' OR "STATUS" = '03')
```

**Implementation**: Pattern matching in `_apply_catalog_rewrites()`

#### Rule 3: Legacy Function Rewrites (Priority 10)

**Context**: HANA 1.x helper functions not recognized in HANA 2.x

**Transformations**:
```sql
-- STRING() ‚Üí TO_VARCHAR()
string("VBELN") ‚Üí TO_VARCHAR("VBELN")

-- LEFTSTR() ‚Üí SUBSTRING()
leftstr("CALMONTH", 4) ‚Üí SUBSTRING("CALMONTH", 1, 4)

-- RIGHTSTR() ‚Üí RIGHT()
rightstr("CALMONTH", 2) ‚Üí RIGHT("CALMONTH", 2)

-- match() ‚Üí REGEXP_LIKE()
match("EMAIL", '.*@example\\.com') ‚Üí REGEXP_LIKE("EMAIL", '.*@example\\.com')
```

**Implementation**: Function catalog (`functions.yaml`)

#### Rule 4: CURRENT_TIMESTAMP() Parentheses (Priority 20)

**Context**: HANA expects `CURRENT_TIMESTAMP` without parentheses when called with no arguments

**Transformation**:
```sql
-- Before
DAYS_BETWEEN(ERDAT, CURRENT_TIMESTAMP())

-- After
DAYS_BETWEEN(ERDAT, CURRENT_TIMESTAMP)
```

**Implementation**: Catalog template handler

#### Rule 5: Schema-Qualified View Creation (Priority 5)

**Context**: Views must be created with explicit schema

**Transformation**:
```sql
-- Before (invalid)
CREATE VIEW CV_CNCLD_EVNTS AS ...

-- After (valid)
DROP VIEW "_SYS_BIC"."Package.Path/CV_CNCLD_EVNTS" CASCADE;
CREATE VIEW "_SYS_BIC"."Package.Path/CV_CNCLD_EVNTS" AS ...
```

**Implementation**: `_render_hana_sql()` with package mapping

#### Rule 6: Calculation View References (CRITICAL)

**Context**: CV-to-CV references must use `"_SYS_BIC"."Package/CV_NAME"` format

**Problem**:
```sql
-- WRONG
INNER JOIN eligibility__cv_md_eyposper ON ...
-- Error: Could not find table/view

-- CORRECT
INNER JOIN "_SYS_BIC"."Macabi_BI.Eligibility/CV_MD_EYPOSPER" ON ...
```

**Implementation**: `_render_from()` with `DataSourceType.CALCULATION_VIEW` detection

**Bug Reference**: BUG-025 (PRINCIPLE #1)

#### Rule 7: Join Table Schema Qualification (Priority 25)

**Context**: Direct table references in joins need schema prefix

**Transformation**:
```sql
-- Before (invalid)
FROM _bic_azekko2
RIGHT OUTER JOIN union_1 ON ...

-- After (valid)
FROM SAPABAP1."/BIC/AZEKKO2" AS _bic_azekko2
RIGHT OUTER JOIN union_1 AS union_1 ON ...
```

**Implementation**: `_render_join()` using `_render_from()` for both inputs

**Bug Reference**: BUG-028

#### Rule 8: Empty String ‚Üí NULL (Priority 45)

**Context**: Empty string literals should be converted to NULL for type safety

**Transformation**:
```sql
-- Before
CASE WHEN ... THEN 'value' ELSE '' END

-- After
CASE WHEN ... THEN 'value' ELSE NULL END
```

**Implementation**: Pattern matching in expression renderer

#### Rule 9: Parameter Cleanup (Priority 80)

**Context**: Remove parameter patterns after substitution

**Transformations**:
```sql
-- Pattern 1: Empty string IN numeric
WHERE '' IN (0) OR column IN (...)  ‚Üí  WHERE (column IN (...))

-- Pattern 2: Empty WHERE clause
WHERE ()  ‚Üí  (omit WHERE clause entirely)
```

**Implementation**: `_cleanup_hana_parameter_conditions()` in renderer

**Bug References**: BUG-021, BUG-022

#### Rule 10: Column Qualification in JOINs (Priority 70)

**Context**: Calculated columns in JOINs must be qualified with table alias

**Transformation**:
```sql
-- Before (ambiguous)
SELECT CALC_COLUMN FROM join_1

-- After (qualified)
SELECT join_1.CALC_COLUMN FROM join_1
```

**Implementation**: `_qualify_columns()` in renderer

### Snowflake Conversion Rules (Top 5)

#### Rule S1: IF() to IFF()

```sql
-- HANA
IF(condition, then_value, else_value)

-- Snowflake
IFF(condition, then_value, else_value)
```

#### Rule S2: String Concatenation

```sql
-- HANA
'Hello' + ' ' + 'World'

-- Snowflake
'Hello' || ' ' || 'World'
```

#### Rule S3: Date Functions

```sql
-- HANA
ADD_DAYS(CURRENT_DATE, -30)

-- Snowflake
DATEADD(DAY, -30, CURRENT_DATE)
```

#### Rule S4: Identifier Quoting

```sql
-- HANA
"COLUMN_NAME"

-- Snowflake
COLUMN_NAME  (unquoted, or "COLUMN_NAME" with different semantics)
```

#### Rule S5: Schema Qualification

```sql
-- HANA
SAPABAP1.VBAK

-- Snowflake
DATABASE.SCHEMA.VBAK
```

---

## Multi-Database Support Strategy

### Mode-Based Architecture

**Two Database Modes**:
1. **HANA Mode** (`database_mode: hana`):
   - Generate native HANA SQL views
   - Use HANA-specific functions (DAYS_BETWEEN, ADD_DAYS, etc.)
   - Create views in `_SYS_BIC` catalog
   - Support version-specific syntax (1.0, 2.0, SPS01, SPS03)

2. **Snowflake Mode** (`database_mode: snowflake`):
   - Generate Snowflake SQL
   - Use Snowflake functions (IFF, DATEADD, etc.)
   - Different identifier quoting rules
   - Different date/time handling

### Mode Selection

**Configuration** (`config.yaml`):
```yaml
defaults:
  database_mode: "hana"  # or "snowflake"
  hana_version: "2.0"    # if hana mode

scenarios:
  - id: "CV_EXAMPLE"
    database_mode: "hana"  # override per-scenario
```

**CLI**:
```bash
xml-to-sql convert --config config.yaml --mode hana
xml-to-sql convert --config config.yaml --mode snowflake --scenario CV_EXAMPLE
```

**API**:
```python
config = ConversionConfig(
    database_mode="hana",
    hana_version="2.0"
)
result = convert_xml_to_sql(xml_content, config)
```

### Function Translation Strategy

**Catalog-Driven Approach**:

**functions.yaml**:
```yaml
- name: IF
  handler: mode_specific
  hana:
    template: "CASE WHEN {0} THEN {1} ELSE {2} END"
  snowflake:
    template: "IFF({0}, {1}, {2})"
  description: "Conditional expression"

- name: LEFTSTR
  handler: mode_specific
  hana:
    template: "SUBSTRING({0}, 1, {1})"
  snowflake:
    template: "LEFT({0}, {1})"
```

**Runtime Resolution**:
```python
def translate_function(func_name, args, mode):
    catalog_entry = catalog.get(func_name)

    if catalog_entry.handler == "mode_specific":
        template = catalog_entry[mode]["template"]
        return template.format(*args)
```

### Version-Aware Generation (HANA)

**HANA Versions Supported**:
- **1.0**: Legacy syntax, older functions
- **2.0**: Modern syntax, new functions
- **2.0 SPS01**: Additional features
- **2.0 SPS03**: Latest features

**Version-Specific Logic**:
```python
if hana_version == "1.0":
    # Use legacy syntax
    sql = "... LEFTSTR(...)"
elif hana_version >= "2.0":
    # Use modern syntax
    sql = "... SUBSTRING(...)"
```

### Comparison: HANA vs Snowflake

| Feature | HANA | Snowflake |
|---------|------|-----------|
| **Conditional** | `CASE WHEN ... END` | `IFF(...)` |
| **String Concat** | `+` | `\|\|` |
| **Date Add** | `ADD_DAYS(date, n)` | `DATEADD(DAY, n, date)` |
| **Substring** | `SUBSTRING(str, pos, len)` | `SUBSTRING(str, pos, len)` |
| **Case** | Case-sensitive "COLUMN" | Case-insensitive COLUMN |
| **Schema** | `SCHEMA."TABLE"` | `DATABASE.SCHEMA.TABLE` |
| **View Location** | `_SYS_BIC."Package/View"` | `DATABASE.SCHEMA.VIEW` |

### Extensibility to Other Databases

**Adding PostgreSQL Support** (hypothetical):

**1. Add mode to config**:
```yaml
defaults:
  database_mode: "postgresql"
```

**2. Extend function catalog**:
```yaml
- name: IF
  handler: mode_specific
  hana: "CASE WHEN {0} THEN {1} ELSE {2} END"
  snowflake: "IFF({0}, {1}, {2})"
  postgresql: "CASE WHEN {0} THEN {1} ELSE {2} END"
```

**3. Implement renderer**:
```python
def _render_postgresql_sql(scenario, config):
    # PostgreSQL-specific SQL generation
    pass
```

**4. Handle dialect differences**:
- Identifier quoting (PostgreSQL uses lowercase by default)
- Schema qualification (`schema.table`)
- Function names (`CONCAT` vs `||`)

---

## Package Mapping System

### Problem Statement

**Challenge**: HANA Calculation Views stored in package hierarchies

**Example**:
- **CV Storage**: `Content > Macabi_BI > Eligibility > CV_MD_EYPOSPER`
- **SQL Reference**: `"_SYS_BIC"."Macabi_BI.Eligibility/CV_MD_EYPOSPER"`

**Manual Specification Issues**:
- Error-prone: Typos in package paths
- Time-consuming: Look up package for each CV
- Inconsistent: Different naming conventions across instances

### Solution: Automatic Package Resolution

**Metadata Sources**:
1. **Excel Export**: Export HANA CV metadata to xlsx
2. **Database Query**: SQL query against HANA system catalog
3. **JSON Cache**: Pre-computed mappings for performance

**Data Flow**:
```
HANA Instance (_SYS_REPO catalog)
         ‚Üì
   [Excel Export]
         ‚Üì
excel_export.xlsx (CV_NAME, PACKAGE_ID columns)
         ‚Üì
   [import_from_excel.py]
         ‚Üì
package_mappings.json
         ‚Üì
 [PackageMapper singleton]
         ‚Üì
Runtime lookup: CV_NAME ‚Üí Package.Path
```

### Implementation

**PackageMapper Class** (`src/xml_to_sql/package_mapper.py`):

```python
class PackageMapper:
    """Singleton mapper for CV name ‚Üí HANA package path"""

    def __init__(self):
        self._mappings = {}
        self._load_mappings()

    def get_package(self, cv_name: str) -> Optional[str]:
        """Get package path for CV name (case-insensitive)"""
        # Direct match
        if cv_name in self._mappings:
            return self._mappings[cv_name]

        # Case-insensitive match
        cv_upper = cv_name.upper()
        for name, pkg in self._mappings.items():
            if name.upper() == cv_upper:
                return pkg.strip()

        return None
```

**Singleton Pattern**:
```python
_mapper: Optional[PackageMapper] = None

def get_package(cv_name: str) -> Optional[str]:
    """Get package for CV (global singleton)"""
    global _mapper
    if _mapper is None:
        _mapper = PackageMapper()
    return _mapper.get_package(cv_name)
```

### Usage Examples

**CLI**:
```bash
# Auto-detect package from filename
xml-to-sql convert --config config.yaml --mode hana

# Manual override (if auto-detection fails)
xml-to-sql convert --config config.yaml --mode hana --hana-package "Custom.Package"
```

**Python API**:
```python
from xml_to_sql.package_mapper import get_package

package = get_package("CV_CNCLD_EVNTS")
# Returns: "EYAL.EYAL_CTL"

# Use in SQL generation
sql = f'CREATE VIEW "_SYS_BIC"."{package}/{cv_name}" AS ...'
```

**Web API** (automatic integration):
```javascript
// Frontend sends XML filename
fetch("/api/convert/single", {
  method: "POST",
  body: formData  // Contains file: CV_CNCLD_EVNTS.xml
})

// Backend auto-detects package:
// 1. Extract CV name: "CV_CNCLD_EVNTS"
// 2. Query mapper: get_package("CV_CNCLD_EVNTS")
// 3. Returns: "EYAL.EYAL_CTL"
// 4. Generate SQL with correct package path
```

### Validation Results

**Test Suite**: `test_package_mapper.py`

**Results**: ‚úÖ **8/8 CVs correctly mapped (100%)**

| CV Name | Expected Package | Actual Package | Status |
|---------|------------------|----------------|--------|
| CV_CNCLD_EVNTS | EYAL.EYAL_CTL | EYAL.EYAL_CTL | ‚úÖ |
| CV_INVENTORY_ORDERS | EYAL.EYAL_CTL | EYAL.EYAL_CTL | ‚úÖ |
| CV_PURCHASE_ORDERS | EYAL.EYAL_CTL | EYAL.EYAL_CTL | ‚úÖ |
| CV_EQUIPMENT_STATUSES | EYAL.EYAL_CTL | EYAL.EYAL_CTL | ‚úÖ |
| CV_MCM_CNTRL_Q51 | EYAL.EYAL_CTL | EYAL.EYAL_CTL | ‚úÖ |
| CV_MCM_CNTRL_REJECTED | EYAL.EYAL_CTL | EYAL.EYAL_CTL | ‚úÖ |
| CV_CT02_CT03 | EYAL.EYAL_CTL | EYAL.EYAL_CTL | ‚úÖ |
| CV_COMMACT_UNION | Macabi_BI.COOM | Macabi_BI.COOM | ‚úÖ |

### Future Enhancements

1. **Multi-Instance Support**: Store mappings per HANA instance (ECC, BW, MBD, BID)
2. **Fuzzy Matching**: Handle CV name variations (CV_NAME vs cv_name)
3. **Package Validation**: Verify package exists in target HANA system
4. **Web UI Explorer**: Visual browser for package hierarchy
5. **Auto-Update**: Sync mappings with HANA system changes

---

## Validation & Quality Assurance

### Testing Methodology

**Multi-Phase Validation**:

**Phase 1: Unit Tests** (pytest):
- Test IR object creation
- Test expression parsing
- Test function translation
- Test SQL rendering

**Phase 2: Integration Tests**:
- Parse XML ‚Üí IR ‚Üí SQL (end-to-end)
- Validate IR structure
- Compare SQL output with snapshots

**Phase 3: HANA Execution Tests**:
- Execute generated SQL in HANA Studio
- Verify view creation succeeds
- Measure execution time
- Document any errors

**Phase 4: Regression Tests**:
- Re-test all previously validated XMLs
- Ensure no breaking changes
- Maintain golden SQL snapshots

### Validation Process

**Empirical Testing Cycle** (doc: `EMPIRICAL_TESTING_CYCLE.md`):

```
1. SELECT XML
   ‚Üì
2. CONVERT (Web UI or CLI)
   ‚Üì
3. EXECUTE in HANA Studio
   ‚Üì
4. [SUCCESS] ‚Üí Document + Move to next XML
   ‚Üì
5. [FAILURE] ‚Üí Create BUG ticket
   ‚Üì
6. ANALYZE error (MANDATORY_PROCEDURES.md)
   ‚Üì
7. FIX bug (minimal changes)
   ‚Üì
8. RE-TEST (regression test all validated XMLs)
   ‚Üì
9. DOCUMENT solution in SOLVED_BUGS.md
   ‚Üì
10. COMMIT changes
```

### Mandatory Procedures (`.claude/MANDATORY_PROCEDURES.md`)

**Bug Tracking Workflow** (NON-NEGOTIABLE):

**Step 1: CHECK EXISTING BUGS FIRST**
- Read `BUG_TRACKER.md`
- Read `SOLVED_BUGS.md`
- Search for error message
- Avoid duplicate work

**Step 2: CREATE BUG TICKET IF NEW**
- Assign BUG-XXX number (sequential)
- Document error message
- Show SQL fragment with issue
- Analyze root cause
- Propose solution

**Step 3: IMPLEMENT FIX**
- Make minimal code changes (surgical precision)
- Test with affected XML
- Document changes inline

**Step 4: REGRESSION TESTING**
- Test ALL previously validated XMLs
- If any breaks, REVERT immediately
- Working code > new fix

**Step 5: UPDATE DOCUMENTATION**
- Move bug to SOLVED_BUGS.md
- Add solution details
- Update bug statistics

**Step 6: BUG ID PRESERVATION** (NEW as of Session 9):
- BUG-XXX ID is PERMANENT
- Code comments use original ID: `# BUG-029: fix reason`
- Git commits reference original ID: `BUGFIX: BUG-029 - description`
- SOLVED_BUGS.md keeps original ID (NOT renamed to SOLVED-029)
- NEVER renumber bugs

### Bug Resolution Statistics

**Total Bugs**: 28 documented
- **Solved**: 23 bugs (82%)
- **Active**: 5 bugs (18%)
- **Deferred**: 2 bugs (parameter patterns)

**Resolution Time**:
- **Fast**: < 1 hour (10 bugs)
- **Medium**: 1-3 hours (8 bugs)
- **Complex**: > 3 hours (5 bugs)

**Bug Categories**:
- Function translation: 8 bugs (35%)
- Schema qualification: 5 bugs (22%)
- Expression rendering: 4 bugs (17%)
- Parameter handling: 3 bugs (13%)
- XML parsing: 3 bugs (13%)

### Validation Tools

**1. SQL Validator** (`src/xml_to_sql/sql/validator.py`):
- **Structure Validation**: CTEs, SELECT, balanced quotes
- **Completeness**: Missing node references, undefined CTEs
- **Performance**: Cartesian products, SELECT *, missing WHERE
- **Mode-Specific**: HANA/Snowflake syntax checks

**2. Auto-Corrector** (`src/xml_to_sql/sql/corrector.py`):
- **High-Confidence Fixes**: Reserved keyword quoting, string concat
- **Medium-Confidence**: Schema qualification (placeholders)
- **Correction Display**: Before/after diff, line numbers

**3. Regression Test Script** (`regression_test.py`):
```bash
cd xml2sql
python regression_test.py --compare-with-golden
```

### Quality Metrics

**Code Quality**:
- **Test Coverage**: 75% (target: 85%)
- **Type Hints**: 90% of functions
- **Documentation**: Every public function has docstring

**SQL Quality**:
- **Syntax Errors**: 0 (all validated XMLs)
- **Performance**: < 250ms execution for all tested views
- **Readability**: CTEs with descriptive names, proper indentation

---

## Technical Challenges Solved

### Challenge 1: String Column Type Conversion (BUG-026)

**Problem**:
XML filter value `= 01` generated unquoted numeric literal for **string column** CODAPL:
```sql
WHERE CODAPL = 01  -- WRONG: 01 is integer, CODAPL is VARCHAR
```

**Error**:
```
attribute value is not a number: int("CODAPL") = 01, CODAPL = ''[string]
```

**Root Cause**:
SAP code columns (CODAPL, MANDT, BUKRS) often use **CHAR/VARCHAR types** even though values look numeric (`'01'`, `'02'`). XML filter values with leading zeros (`01`) were treated as integers.

**Solution**:
Enhanced filter value handling to detect leading zeros and add quotes:
```python
# In parameter substitution
if value.startswith('0') and value.isdigit():
    return f"'{value}'"  # Quote it
```

**Result**: CURRENT_MAT_SORT.xml now executes successfully.

**Lesson**: When in doubt, QUOTE IT. SAP columns use strings for codes.

---

### Challenge 2: Join Table Schema Qualification (BUG-028)

**Problem**:
Direct table entity inputs in JOINs generated bare aliases without schema:
```sql
FROM _bic_azekko2  -- WRONG: No schema
RIGHT OUTER JOIN union_1 ON ...
```

**Error**:
```
Could not find table/view _BIC_AZEKKO2 in schema _SYS_BIC
```

**Root Cause**:
`_render_join()` used `ctx.get_cte_alias()` for both inputs, which just lowercases node IDs. This works for CTE references but fails for direct table references.

**Solution**:
Use `_render_from()` for both join inputs:
```python
# Get proper FROM clauses for both CTEs and tables
left_from = _render_from(ctx, left_id)   # Returns: SAPABAP1."/BIC/AZEKKO2"
right_from = _render_from(ctx, right_id)  # Returns: union_1

# Add AS aliases for column references
sql = f"FROM {left_from} AS {left_alias} JOIN {right_from} AS {right_alias} ON ..."
```

**Result**:
```sql
FROM SAPABAP1."/BIC/AZEKKO2" AS _bic_azekko2
RIGHT OUTER JOIN union_1 AS union_1 ON _bic_azekko2.EBELN = union_1.REF_DOC_NR
```

**Validation**: CV_COMMACT_UNION.xml executes in 36ms.

---

### Challenge 3: Calculation View Reference Paths (BUG-025)

**Problem**:
CV-to-CV references used incorrect schema format:
```sql
INNER JOIN eligibility__cv_md_eyposper ON ...  -- WRONG: Lowercase alias
```

**Error**:
```
Could not find table/view ELIGIBILITY__CV_MD_EYPOSPER in schema _SYS_BIC
```

**Root Cause - FUNDAMENTAL ARCHITECTURAL PRINCIPLE**:

HANA has TWO separate storage locations that must NEVER be confused:

1. **HANA CV Storage** (Source):
   - Location: `Content > Macabi_BI > Eligibility`
   - Where CV definitions live
   - Format: Package hierarchy with dots

2. **SQL View Location** (Target):
   - Location: `Systems > _SYS_BIC > Views`
   - Where generated SQL views are created
   - Format: `"_SYS_BIC"."Package.Path/CV_NAME"`

**Solution**:
Detect CALCULATION_VIEW data sources and use proper reference format:
```python
if ctx.database_mode == DatabaseMode.HANA and ds.source_type == DataSourceType.CALCULATION_VIEW:
    from ..package_mapper import get_package
    cv_name = ds.object_name
    package = get_package(cv_name)
    if package:
        view_name_with_package = f"{package}/{cv_name}"
        return f'"_SYS_BIC".{_quote_identifier(view_name_with_package)}'
```

**Result**:
```sql
INNER JOIN "_SYS_BIC"."Macabi_BI.Eligibility/CV_MD_EYPOSPER" ON ...
```

**Impact**: Critical fix affecting ANY XML with CV-to-CV references.

---

### Challenge 4: Legacy Function Translation (BUG-013, BUG-017)

**Problem**:
HANA 1.x helper functions not recognized in HANA 2.x:
```sql
WHERE string("VBELN") = '001'  -- ERROR: invalid function STRING
```

**Root Cause**:
Function catalog missing mappings for legacy helpers.

**Solution**:
Expand function catalog with systematic mappings:
```yaml
functions:
  - name: STRING
    handler: rename
    target: "TO_VARCHAR"

  - name: INT
    handler: rename
    target: "TO_INTEGER"

  - name: LEFTSTR
    handler: template
    template: "SUBSTRING({0}, 1, {1})"

  - name: RIGHTSTR
    handler: rename
    target: "RIGHT"
```

**Result**: All legacy functions automatically translated during formula processing.

---

### Challenge 5: Empty WHERE Clause After Parameter Cleanup (BUG-022)

**Problem**:
After parameter substitution cleanup, empty WHERE clauses remained:
```sql
WHERE ()  -- SYNTAX ERROR
```

**Root Cause**:
Parameter pattern `'' IN (0)` removed by BUG-021 cleanup, leaving empty parentheses.

**Solution**:
Post-cleanup validation in 6 rendering functions:
```python
# After parameter cleanup
if cleaned_sql.strip().endswith("WHERE ()") or cleaned_sql.strip().endswith("WHERE (  )"):
    # Remove empty WHERE clause
    cleaned_sql = re.sub(r'\s*WHERE\s*\(\s*\)\s*$', '', cleaned_sql)
```

**Result**: CV_MCM_CNTRL_REJECTED.xml executes in 53ms.

---

## Extension Opportunities

### 1. Additional Database Targets

**PostgreSQL**:
- **Similarity**: 80% compatible with HANA SQL
- **Differences**: Identifier quoting, function names
- **Effort**: 2-3 weeks for full support
- **Use Case**: Open-source alternative to HANA

**Databricks**:
- **Similarity**: Spark SQL dialect (similar to Hive)
- **Differences**: Different date functions, window function syntax
- **Effort**: 3-4 weeks
- **Use Case**: Lakehouse analytics

**BigQuery**:
- **Similarity**: Standard SQL with extensions
- **Differences**: Different string functions, no CTEs in views
- **Effort**: 4-5 weeks
- **Use Case**: Google Cloud integration

**Redshift**:
- **Similarity**: PostgreSQL-based
- **Differences**: Older PostgreSQL version, performance hints
- **Effort**: 2-3 weeks
- **Use Case**: AWS data warehouse

### 2. Other SAP Object Types

**ABAP Code**:
- **Challenge**: Convert ABAP programs to Python/SQL
- **IR Approach**: Abstract ABAP constructs (SELECT, LOOP, IF)
- **Target**: Python data pipelines
- **Use Case**: Modernize legacy ABAP logic

**BW Queries**:
- **Challenge**: BEx query XML ‚Üí SQL
- **IR Approach**: Query structure (characteristics, key figures, filters)
- **Target**: SQL views or BI tool queries
- **Use Case**: Migrate BW analytics to cloud

**Data Services Jobs**:
- **Challenge**: SAP Data Services XML ‚Üí ETL pipelines
- **IR Approach**: Data flow (sources, transformations, targets)
- **Target**: Airflow DAGs, dbt models, Spark jobs
- **Use Case**: Modernize ETL processes

**Transformation Rules**:
- **Challenge**: SAP transformation rules ‚Üí SQL CASE statements
- **IR Approach**: Rule conditions and mappings
- **Target**: SQL scalar functions
- **Use Case**: Code conversion lookup tables

### 3. Metadata-Driven Systems

**General Approach**:
The IR-based architecture is applicable to ANY system that stores logic in structured metadata:

**Pattern**:
```
Metadata (XML/JSON/YAML)
         ‚Üì
    [Parser]
         ‚Üì
Intermediate Representation
         ‚Üì
   [Renderer]
         ‚Üì
Executable Code (SQL/Python/Java)
```

**Examples**:

**1. Informatica PowerCenter**:
- Metadata: Workflow XML
- IR: Data flow graph
- Target: Spark/Airflow jobs

**2. Oracle Forms**:
- Metadata: FMB files
- IR: Form structure, triggers
- Target: Web UI (React) + API

**3. BPEL Processes**:
- Metadata: BPEL XML
- IR: Process flow
- Target: Microservices orchestration

**4. Talend Jobs**:
- Metadata: Talend XML
- IR: Job components
- Target: Spark/Flink jobs

### 4. Pattern Matching Generalization

**Current System**: `patterns.yaml` for expression rewrites

**Extension Opportunity**: Generalize to other transformation domains

**Use Cases**:

**Code Migration**:
```yaml
patterns:
  - pattern: "System.out.println\\((.*)\\)"
    replacement: "logger.info(\\1)"
    language: "java"
```

**SQL Dialect Translation**:
```yaml
patterns:
  - pattern: "DATEADD\\(DAY, (\\d+), (.*)\\)"
    replacement: "ADD_DAYS(\\2, \\1)"
    source: "snowflake"
    target: "hana"
```

**Legacy Code Modernization**:
```yaml
patterns:
  - pattern: "SELECT \\* FROM"
    replacement: "SELECT explicit_columns FROM"
    confidence: "HIGH"
```

### 5. Advanced Features

**Query Optimization**:
- Analyze IR for optimization opportunities
- Suggest indexes based on JOIN conditions
- Identify expensive operations (Cartesian products)
- Generate execution plan annotations

**Security Analysis**:
- Extract analytic privilege constraints from IR
- Generate row-level security policies for target DB
- Audit sensitive column access
- Generate data masking rules

**Documentation Generation**:
- Auto-generate data lineage diagrams from IR
- Create data dictionary from logical model
- Generate business-friendly descriptions
- Export as Markdown/HTML/PDF

**Testing Framework**:
- Generate test data based on IR structure
- Create unit tests for calculated columns
- Validate business logic consistency
- Generate integration test suites

---

## Reference Documentation

### Source Documents (Full Context)

This research brief synthesizes content from the following source documents. For complete technical details, consult these files:

#### 1. **Primary Reference** (Start Here)
- **File**: `xml2sql/README.md`
- **Purpose**: Project overview, installation, quick start
- **Best For**: Understanding what the project does in 15 minutes

#### 2. **Authoritative Handover** (Most Complete)
- **File**: `xml2sql/docs/llm_handover.md`
- **Size**: 1,892 lines
- **Purpose**: Complete project state, implementation notes, session history
- **Best For**: Understanding current status, validated XMLs, bug history
- **Note**: Updated after every session with latest status

#### 3. **Architecture Details**
- **File**: `xml2sql/docs/CONVERSION_FLOW_MAP.md`
- **Purpose**: Parser pipeline, SQL generation flow, component responsibilities
- **Best For**: Understanding data flow from XML ‚Üí IR ‚Üí SQL

#### 4. **IR Design Philosophy**
- **File**: `xml2sql/docs/ir_design.md`
- **Purpose**: Intermediate representation structure, extensibility hooks
- **Best For**: Understanding the abstraction layer enabling multi-database support

#### 5. **Conversion Rules** (Critical)
- **File**: `xml2sql/docs/rules/HANA_CONVERSION_RULES.md`
- **Size**: 17+ transformation rules
- **Purpose**: Complete specification of XML ‚Üí HANA SQL transformations
- **Best For**: Understanding exact conversion logic, validation evidence

- **File**: `xml2sql/docs/rules/SNOWFLAKE_CONVERSION_RULES.md`
- **Purpose**: Snowflake-specific transformations (5 rules)
- **Best For**: Understanding Snowflake mode differences

#### 6. **Bug Documentation** (Lessons Learned)
- **File**: `xml2sql/docs/bugs/BUG_TRACKER.md`
- **Purpose**: Active bugs with root cause analysis
- **Best For**: Understanding current limitations

- **File**: `xml2sql/docs/bugs/SOLVED_BUGS.md`
- **Size**: 23 solved bugs
- **Purpose**: Complete solutions with code changes, validation results
- **Best For**: Learning from past challenges, understanding solved patterns

#### 7. **Implementation Guides**
- **File**: `xml2sql/docs/implementation/PATTERN_MATCHING_DESIGN.md`
- **Purpose**: Pattern matching system architecture
- **Best For**: Understanding expression rewrite engine

- **File**: `xml2sql/docs/implementation/AUTO_CORRECTION_TESTING_GUIDE.md`
- **Purpose**: Auto-correction feature testing methodology

#### 8. **Development Procedures**
- **File**: `xml2sql/.claude/MANDATORY_PROCEDURES.md`
- **Purpose**: Bug checking, SQL analysis, code change procedures (NON-NEGOTIABLE)
- **Best For**: Understanding quality assurance workflow

- **File**: `xml2sql/.claude/claude.md`
- **Purpose**: 18 mandatory behavior rules for LLM development
- **Best For**: Understanding development standards

### Recommended Reading Order

**For Quick Understanding** (1 hour):
1. README.md (15 min)
2. llm_handover.md - Section: "Current State" and "Validated XMLs" (20 min)
3. HANA_CONVERSION_RULES.md - Top 10 rules (25 min)

**For Complete Understanding** (2-3 hours):
1. README.md
2. llm_handover.md (skim session history, focus on latest)
3. CONVERSION_FLOW_MAP.md
4. ir_design.md
5. HANA_CONVERSION_RULES.md
6. SOLVED_BUGS.md (top 5 bugs)

**For Extension Research** (Deep Dive):
1. All above documents
2. PATTERN_MATCHING_DESIGN.md
3. BUG_TRACKER.md (active bugs)
4. Source code review:
   - `src/xml_to_sql/parser/scenario_parser.py`
   - `src/xml_to_sql/sql/renderer.py`
   - `src/xml_to_sql/sql/function_translator.py`

### Key Statistics Summary

**Project Maturity**:
- **Age**: ~3 months of active development
- **Sessions**: 8 major development sessions documented
- **Commits**: 50+ commits with structured messages
- **Test Coverage**: 75% (target: 85%)

**Validation Evidence**:
- **XMLs Tested**: 9 production calculation views
- **Success Rate**: 100% (8/8 executed successfully in HANA)
- **Execution Times**: 29ms to 243ms per view
- **Bugs Resolved**: 23/28 (82%)
- **Code Quality**: All generated SQL validates in HANA Studio

**Technical Complexity**:
- **XML Complexity**: Up to 2,139 lines (CV_TOP_PTHLGY)
- **SQL Output**: Up to 630 lines generated SQL
- **Node Count**: Up to 12 nodes per calculation view
- **JOIN Depth**: Up to 10+ joins in complex views

### Research Directions

Based on this comprehensive context, promising research directions include:

1. **Database Extension**: Adding PostgreSQL, Databricks, BigQuery support
2. **SAP Object Coverage**: Expanding to BW queries, ABAP code, Data Services
3. **Pattern Generalization**: Applying pattern matching to other domains
4. **Optimization**: Query optimization based on IR analysis
5. **Security**: Analytic privilege propagation and row-level security
6. **Testing**: Automated test generation from IR structure
7. **Documentation**: Auto-generation of lineage and data dictionaries
8. **Performance**: Distributed processing for large XML batches

### Contact & Contribution

**Repository**: https://github.com/iliyaruvinsky/xml2sql
**Version**: 2.2.0 (HANA mode production-ready)
**Status**: Active development, production validated

**Key Maintainer**: Iliya Ruvinsky
**Development Approach**: LLM-assisted (Claude Code) with human validation
**Quality Standard**: 100% HANA execution validation required

---

**Document Version**: 3.0.0
**Generated**: 2025-11-21
**Purpose**: Pre-research context for external Claude agent
**Next Update**: After Session 10 (next major milestone)

---

## Appendix: Quick Reference Tables

### Function Translation Quick Reference

| HANA Function | Snowflake | PostgreSQL | Description |
|---------------|-----------|------------|-------------|
| `IF(cond, then, else)` | `IFF(...)` | `CASE WHEN ... END` | Conditional |
| `LEFTSTR(str, n)` | `LEFT(str, n)` | `LEFT(str, n)` | Left substring |
| `RIGHTSTR(str, n)` | `RIGHT(str, n)` | `RIGHT(str, n)` | Right substring |
| `STRING(x)` | `TO_VARCHAR(x)` | `CAST(x AS VARCHAR)` | Type cast |
| `INT(x)` | `TO_NUMBER(x)` | `CAST(x AS INTEGER)` | Type cast |
| `ADD_DAYS(date, n)` | `DATEADD(DAY, n, date)` | `date + INTERVAL 'n days'` | Date arithmetic |
| `str1 + str2` | `str1 \|\| str2` | `str1 \|\| str2` | String concat |

### Node Type Rendering Reference

| Node Type | SQL Pattern | Example |
|-----------|-------------|---------|
| **Projection** | `SELECT cols FROM source WHERE ...` | Select + calc columns |
| **Join** | `SELECT ... FROM left JOIN right ON ...` | Inner/outer joins |
| **Aggregation** | `SELECT ..., AGG(col) FROM ... GROUP BY ...` | SUM, MAX, COUNT |
| **Union** | `SELECT ... FROM a UNION ALL SELECT ... FROM b` | Combine results |
| **Rank** | `SELECT ..., ROW_NUMBER() OVER (...) FROM ...` | Top N |

### Error Pattern Quick Reference

| Error Message | Likely Cause | BUG Reference |
|---------------|--------------|---------------|
| `invalid function: STRING` | Legacy function | BUG-013 |
| `invalid schema name: ABAP` | Missing schema qualification | BUG-014 |
| `attribute value is not a number` | Unquoted string value | BUG-026 |
| `Could not find table/view` | Wrong CV reference format | BUG-025 |
| `incorrect syntax near ')'` | Empty WHERE clause | BUG-022 |

---

**End of Research Brief**
